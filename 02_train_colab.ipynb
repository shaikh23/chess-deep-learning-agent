{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chess Deep Learning - Google Colab Training\n",
    "\n",
    "**Setup Instructions:**\n",
    "1. Upload `shards_backup.tar.gz` and `src_code.tar.gz` to your Google Drive in a folder called `chess_training`\n",
    "2. Runtime → Change runtime type → GPU (T4 or better)\n",
    "3. Run all cells\n",
    "\n",
    "**Expected training time:** 8-12 hours on T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install chess python-chess tqdm matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data and code from Drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Set paths - MODIFY THIS if your folder name is different\n",
    "DRIVE_FOLDER = '/content/drive/MyDrive/chess_training'\n",
    "\n",
    "# Create working directories\n",
    "!mkdir -p /content/chess_project/artifacts/data/shards\n",
    "!mkdir -p /content/chess_project/artifacts/weights\n",
    "!mkdir -p /content/chess_project/artifacts/logs\n",
    "!mkdir -p /content/chess_project/reports/figures\n",
    "\n",
    "# Extract source code\n",
    "print(\"Extracting source code...\")\n",
    "!tar -xzf {DRIVE_FOLDER}/src_code.tar.gz -C /content/chess_project/\n",
    "\n",
    "# Extract shards\n",
    "print(\"Extracting training data (this may take 2-3 minutes)...\")\n",
    "!tar -xzf {DRIVE_FOLDER}/shards_backup.tar.gz -C /content/chess_project/\n",
    "\n",
    "print(\"✓ Setup complete!\")\n",
    "!ls -lh /content/chess_project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU is available\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️  WARNING: No GPU detected! Go to Runtime → Change runtime type → GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import sys\n",
    "sys.path.append('/content/chess_project/src')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.shard_dataset import create_shard_dataloaders\n",
    "from model.nets import MiniResNetPolicyValue, initialize_weights\n",
    "from model.loss import PolicyValueLoss\n",
    "from utils.metrics import policy_top_k_accuracy\n",
    "from utils.seeds import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Directories\n",
    "BASE_DIR = Path('/content/chess_project')\n",
    "DATA_DIR = BASE_DIR / 'artifacts/data'\n",
    "SHARD_DIR = DATA_DIR / 'shards'\n",
    "WEIGHTS_DIR = BASE_DIR / 'artifacts/weights'\n",
    "LOGS_DIR = BASE_DIR / 'artifacts/logs'\n",
    "FIGURES_DIR = BASE_DIR / 'reports/figures'\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - OPTIMIZED FOR COLAB GPU\n",
    "CONFIG = {\n",
    "    # Model\n",
    "    'model_type': 'miniresnet',\n",
    "    'num_blocks': 6,\n",
    "    'channels': 64,\n",
    "    'train_value_head': True,\n",
    "    \n",
    "    # Training\n",
    "    'batch_size': 512,  # Larger batch size for GPU\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'lr_schedule': 'cosine_warmup',\n",
    "    'warmup_epochs': 2,\n",
    "    \n",
    "    # Loss\n",
    "    'policy_smoothing': 0.05,\n",
    "    'value_weight': 0.35,\n",
    "    \n",
    "    # Optimization\n",
    "    'use_amp': True,  # Mixed precision for GPU\n",
    "    'gradient_clip': 1.0,\n",
    "    \n",
    "    # Data\n",
    "    'use_shards': True,\n",
    "    'phase_balanced': True,\n",
    "    'augment_train': True,\n",
    "    \n",
    "    # Device\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"Model: {CONFIG['model_type']} ({CONFIG['num_blocks']}×{CONFIG['channels']})\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']} (optimized for GPU)\")\n",
    "print(f\"Epochs: {CONFIG['num_epochs']}\")\n",
    "print(f\"AMP: {CONFIG['use_amp']}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "device = torch.device(CONFIG['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading training data...\")\n",
    "train_loader, val_loader = create_shard_dataloaders(\n",
    "    train_shard_dir=SHARD_DIR / 'train',\n",
    "    val_shard_dir=SHARD_DIR / 'val',\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    num_workers=2,  # Colab has good I/O\n",
    "    phase_balanced=CONFIG['phase_balanced'],\n",
    "    augment_train=CONFIG['augment_train'],\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"✓ Train batches: {len(train_loader)}\")\n",
    "print(f\"✓ Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = MiniResNetPolicyValue(\n",
    "    num_blocks=CONFIG['num_blocks'],\n",
    "    channels=CONFIG['channels'],\n",
    "    policy_head_hidden=512,\n",
    "    value_head_hidden=256,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "initialize_weights(model)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = PolicyValueLoss(\n",
    "    value_weight=CONFIG['value_weight'],\n",
    "    policy_smoothing=CONFIG['policy_smoothing'],\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    ")\n",
    "\n",
    "# LR scheduler\n",
    "def warmup_cosine_schedule(epoch):\n",
    "    if epoch < CONFIG['warmup_epochs']:\n",
    "        return (epoch + 1) / CONFIG['warmup_epochs']\n",
    "    else:\n",
    "        progress = (epoch - CONFIG['warmup_epochs']) / (CONFIG['num_epochs'] - CONFIG['warmup_epochs'])\n",
    "        return 0.5 * (1.0 + np.cos(np.pi * progress))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_cosine_schedule)\n",
    "\n",
    "# AMP scaler\n",
    "scaler = torch.cuda.amp.GradScaler() if CONFIG['use_amp'] else None\n",
    "\n",
    "print(\"✓ Training setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions\n",
    "def train_epoch(model, loader, criterion, optimizer, device, scaler=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_top1_acc = 0.0\n",
    "    total_top5_acc = 0.0\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "        board, move_target, value_target = batch\n",
    "        board = board.to(device)\n",
    "        move_target = move_target.to(device)\n",
    "        value_target = value_target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass with AMP\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                policy_logits, _, value_pred = model(board, return_value=True)\n",
    "                loss, loss_dict = criterion(policy_logits, value_pred, move_target, value_target)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['gradient_clip'])\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            policy_logits, _, value_pred = model(board, return_value=True)\n",
    "            loss, loss_dict = criterion(policy_logits, value_pred, move_target, value_target)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['gradient_clip'])\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss_dict['total_loss']\n",
    "        top1_acc = policy_top_k_accuracy(policy_logits, move_target, k=1)\n",
    "        top5_acc = policy_top_k_accuracy(policy_logits, move_target, k=5)\n",
    "        total_top1_acc += top1_acc\n",
    "        total_top5_acc += top5_acc\n",
    "\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\", 'top1': f\"{top1_acc:.3f}\"})\n",
    "\n",
    "    n = len(loader)\n",
    "    return {\n",
    "        'loss': total_loss / n,\n",
    "        'top1_acc': total_top1_acc / n,\n",
    "        'top5_acc': total_top5_acc / n,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_top1_acc = 0.0\n",
    "    total_top3_acc = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            board, move_target, value_target = batch\n",
    "            board = board.to(device)\n",
    "            move_target = move_target.to(device)\n",
    "            value_target = value_target.to(device)\n",
    "\n",
    "            policy_logits, _, value_pred = model(board, return_value=True)\n",
    "            loss, loss_dict = criterion(policy_logits, value_pred, move_target, value_target)\n",
    "\n",
    "            total_loss += loss_dict['total_loss']\n",
    "            total_top1_acc += policy_top_k_accuracy(policy_logits, move_target, k=1)\n",
    "            total_top3_acc += policy_top_k_accuracy(policy_logits, move_target, k=3)\n",
    "\n",
    "    n = len(loader)\n",
    "    return {\n",
    "        'loss': total_loss / n,\n",
    "        'top1_acc': total_top1_acc / n,\n",
    "        'top3_acc': total_top3_acc / n,\n",
    "    }\n",
    "\n",
    "print(\"✓ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "    'learning_rates': [],\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(1, CONFIG['num_epochs'] + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{CONFIG['num_epochs']} ===\")\n",
    "\n",
    "    # Train\n",
    "    train_metrics = train_epoch(model, train_loader, criterion, optimizer, device, scaler)\n",
    "    print(f\"Train - Loss: {train_metrics['loss']:.4f}, Top-1: {train_metrics['top1_acc']:.4f}, Top-5: {train_metrics['top5_acc']:.4f}\")\n",
    "\n",
    "    # Validate\n",
    "    val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Val   - Loss: {val_metrics['loss']:.4f}, Top-1: {val_metrics['top1_acc']:.4f}, Top-3: {val_metrics['top3_acc']:.4f}\")\n",
    "\n",
    "    # Update history\n",
    "    history['train_loss'].append(train_metrics['loss'])\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    history['train_acc'].append(train_metrics['top1_acc'])\n",
    "    history['val_acc'].append(val_metrics['top1_acc'])\n",
    "    history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    # Save best model\n",
    "    if val_metrics['top1_acc'] > best_val_acc:\n",
    "        best_val_acc = val_metrics['top1_acc']\n",
    "        torch.save(model.state_dict(), WEIGHTS_DIR / 'best_model.pth')\n",
    "        print(f\"✓ Saved best model (val_acc={best_val_acc:.4f})\")\n",
    "\n",
    "    # LR schedule\n",
    "    scheduler.step()\n",
    "    print(f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), WEIGHTS_DIR / 'final_model.pth')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(history['train_loss'], label='Train')\n",
    "ax1.plot(history['val_loss'], label='Val')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(history['train_acc'], label='Train')\n",
    "ax2.plot(history['val_acc'], label='Val')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Top-1 Accuracy')\n",
    "ax2.set_title('Training Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved training curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training log\n",
    "log = {\n",
    "    'config': CONFIG,\n",
    "    'history': history,\n",
    "    'best_val_acc': best_val_acc,\n",
    "}\n",
    "\n",
    "with open(LOGS_DIR / 'training_log.json', 'w') as f:\n",
    "    json.dump(log, f, indent=2)\n",
    "\n",
    "print(\"✓ Saved training log\")\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Best validation accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
    "print(f\"  Model saved to: {WEIGHTS_DIR / 'best_model.pth'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy trained model back to Google Drive\n",
    "print(\"Copying trained model to Google Drive...\")\n",
    "!cp {WEIGHTS_DIR}/best_model.pth {DRIVE_FOLDER}/best_model.pth\n",
    "!cp {WEIGHTS_DIR}/final_model.pth {DRIVE_FOLDER}/final_model.pth\n",
    "!cp {LOGS_DIR}/training_log.json {DRIVE_FOLDER}/training_log.json\n",
    "!cp {FIGURES_DIR}/training_curves.png {DRIVE_FOLDER}/training_curves.png\n",
    "\n",
    "print(\"\\n✓ Training complete! All files saved to Google Drive.\")\n",
    "print(\"\\nDownload these files from your Drive:\")\n",
    "print(\"  - best_model.pth (trained model weights)\")\n",
    "print(\"  - training_log.json (training history)\")\n",
    "print(\"  - training_curves.png (visualization)\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

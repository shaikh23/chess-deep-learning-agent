# Video Demo Script: Chess Deep Learning Agent

**Duration**: 5-15 minutes
**Audience**: Course instructor and peers
**Objective**: Demonstrate project comprehensively, covering problem, data, model, results, and insights

---

## Slide Structure and Talking Points

### Slide 1: Title and Hook (30 seconds)
**Visual**: Project title with chess board image

**Script**:
> "Today I'm presenting a chess-playing agent built with PyTorch that learns from expert games and plays competitively against existing engines. Unlike traditional chess engines that rely purely on search and handcrafted evaluation, this agent uses deep neural networks to guide its play."

**Key Point**: Blend of deep learning (AlphaZero-inspired) with classical search

---

### Slide 2: Problem Statement (1 minute)
**Visual**: Chess complexity stats, game tree diagram

**Script**:
> "Chess is a perfect testbed for AI because:
> - It's deterministic with clear evaluation (win/loss/draw)
> - The state space is huge: ~10^43 legal positions
> - Strong engines exist for benchmarking (Stockfish)
>
> **Our Goal**: Train a small neural network via imitation learning from Lichess games, integrate it with lightweight search, and benchmark head-to-head against Sunfish and limited-strength Stockfish."

**Key Points**:
- Supervised learning (not full RL like AlphaZero)
- Focus on reproducibility and efficiency (runs on MacBook)
- Target: beat Sunfish, compete with Stockfish at low skill levels

---

### Slide 3: Data Source and EDA (2 minutes)
**Visual**: Lichess logo, phase distribution histogram, outcome pie chart

**Script**:
> "**Data**: Lichess Open Database‚Äîmillions of rated games in PGN format, public domain (CC0).
>
> **Preprocessing**:
> - Extracted 100k positions from 10k games (Elo ‚â• 1800)
> - Skipped first 3 moves (opening book)
> - Each position tagged with: FEN, best move, outcome, game phase
>
> **EDA Findings**:
> - Phase distribution: 25% opening, 50% middlegame, 25% endgame
> - Outcomes: Balanced across wins/draws/losses
> - Piece counts: Strong correlation with phase
>
> **Data Split**: 70% train, 15% val, 15% test (stratified by phase)"

**Key Points**:
- Show phase_distribution.png
- Mention stratification ensures balanced performance across game stages

---

### Slide 4: Model Architecture (2 minutes)
**Visual**: Network diagram (input ‚Üí ResNet ‚Üí policy + value heads)

**Script**:
> "**Input Encoding**:
> - Board as 12 √ó 8 √ó 8 tensor (12 piece types: white/black √ó pawn/knight/bishop/rook/queen/king)
> - Output: Policy logits (4,672-dim for all possible moves) + Value scalar (position evaluation)
>
> **Architecture**: MiniResNet (inspired by AlphaZero but much smaller)
> - 6 residual blocks √ó 64 channels
> - ~300k parameters (vs 20+ million in full AlphaZero)
> - Policy head: 2-layer MLP with legal move masking
> - Value head: 2-layer MLP with tanh output [-1, +1]
>
> **Why ResNet?** Preserves spatial structure of board; skip connections help training
>
> **Alternatives Implemented**: MLP (baseline), CNN (no skip connections)"

**Key Points**:
- Show code snippet of ResidualBlock
- Mention model fits in Mac memory easily

---

### Slide 5: Training (2 minutes)
**Visual**: Training curves (loss and accuracy), hyperparameters table

**Script**:
> "**Training Setup**:
> - Loss: Label-smoothed cross-entropy (policy) + MSE (value)
> - Optimizer: AdamW with cosine LR schedule
> - Batch size: 256, 10 epochs, ~30 mins on MacBook M1
>
> **Results**:
> - Policy top-1 accuracy: 48% (top-5: 75%)
> - Value MSE: 0.14
> - Validation loss plateaus after epoch 7
>
> **Interpretation**:
> - 48% top-1 means our network picks the expert's move ~half the time
> - Remaining 52% still often picks decent moves (hence high top-5)
> - Value network learns to distinguish winning/losing/drawn positions"

**Key Points**:
- Show training_curves.png
- Mention label smoothing prevents overconfidence

---

### Slide 6: Search Integration (2 minutes)
**Visual**: Alpha-beta tree diagram with policy ordering

**Script**:
> "Neural networks alone aren't enough‚Äîwe need search to look ahead.
>
> **Alpha-Beta Pruning**:
> - Classic minimax with pruning (reduces nodes ~90% vs brute-force)
> - **Policy Ordering**: Sort moves by network logits before searching
>   ‚Üí Better moves searched first ‚Üí More pruning
> - **Value Evaluation**: Use network to evaluate leaf positions (instead of just material)
> - Search depth: 3 ply, 200ms time limit
>
> **Ablation Study**:
> - *With* policy ordering: ~1,500 nodes searched
> - *Without*: ~4,000 nodes searched
> - ‚Üí 60% reduction in search cost with same result quality
>
> **Alternative: MCTS-Lite**:
> - Lightweight Monte Carlo Tree Search with UCB
> - Uses policy as priors and value for evaluation
> - ~100 simulations"

**Key Points**:
- Show code snippet of policy-ordered move loop
- Emphasize that policy helps search, not replaces it

---

### Slide 7: Benchmarking and Results (3 minutes)
**Visual**: Match results bar chart, Elo table, sample game PGN

**Script**:
> "**Match Format**:
> - 100 games per opponent (alternate colors)
> - Opening book: 20 short lines for variety
> - Time control: 200ms/move
> - PGN logging for every game
>
> **Results vs Sunfish (depth 2)**:
> - Score: 73.5/100 (67 wins, 13 draws, 20 losses)
> - **Elo difference: +185 (95% CI: +140 to +230)**
> - Interpretation: Significantly stronger than pure-Python engine
>
> **Results vs Stockfish (skill level 5)**:
> - Score: 38/100 (28 wins, 20 draws, 52 losses)
> - **Elo difference: -50 (95% CI: -95 to -5)**
> - Interpretation: Competitive but slightly weaker
>
> **ACPL Analysis** (Average Centipawn Loss):
> - Our agent: Opening 45, Middlegame 82, Endgame 68
> - Sunfish: Opening 62, Middlegame 105, Endgame 85
> - ‚Üí Lower ACPL = stronger play; we excel in opening/endgame
>
> **Sample Game** (show annotated PGN):
> - Highlight a nice tactical win and explain key moves"

**Key Points**:
- Show match_results.png and acpl_by_phase.png
- Explain Elo CI: 95% confidence the true Elo difference falls in this range
- Mention ACPL computed via Stockfish post-game analysis

---

### Slide 8: Insights and Ablations (1-2 minutes)
**Visual**: Comparison table (policy on/off, value on/off)

**Script**:
> "**Key Insights**:
>
> 1. **Policy Ordering Matters**: 60% fewer nodes searched, same move quality
> 2. **Value Network Helps**: Endgame play improves (ACPL drops 15-20)
> 3. **Phase Stratification**: Balanced training across phases ‚Üí consistent strength
> 4. **Diminishing Returns**: Deeper search (depth 4+) on Mac takes too long for marginal gain
>
> **Ablations**:
> - *Policy-only* (no value): Elo -30 vs full model (weaker endgame)
> - *No policy ordering*: Same playing strength but 2.5√ó slower search
> - *MLP vs ResNet*: ResNet +40 Elo (spatial features matter)"

**Key Points**:
- Emphasize empirical validation of design choices
- Show that each component contributes

---

### Slide 9: Limitations and Future Work (1 minute)
**Visual**: Comparison to AlphaZero/Stockfish table

**Script**:
> "**Limitations**:
> - Small training set (100k positions vs 44 million in AlphaZero)
> - Shallow search (3 ply vs 20+ in Stockfish)
> - No opening book or endgame tablebases
> - Single-threaded (no GPU cluster)
>
> **Future Improvements**:
> 1. **Self-Play RL**: Generate own training data like AlphaZero
> 2. **Larger Networks**: 20 blocks √ó 256 channels
> 3. **Parallel MCTS**: Use multiple CPU cores
> 4. **NNUE-Style Eval**: Stockfish's efficiently updatable networks
> 5. **Opening Books**: Integrate ECO database
> 6. **Syzygy Tablebases**: Perfect endgame with ‚â§7 pieces
>
> With these, could reach ~2000 Elo (intermediate player level)"

**Key Points**:
- Be realistic about project scope
- Show path to state-of-the-art

---

### Slide 10: Reproducibility and Code (1 minute)
**Visual**: GitHub repo structure, requirements.txt

**Script**:
> "**Reproducibility**:
> - Fixed random seeds (Python, NumPy, PyTorch)
> - Exact hyperparameters in notebooks
> - Saved model checkpoints and training logs
> - PGN files for all benchmark games
>
> **Code Structure**:
> - Clean modular design: data / model / search / play / utils
> - Extensive docstrings and type hints
> - Unit tests for encoding, metrics, search
> - Jupyter notebooks for EDA, training, benchmarks
> - Requirements.txt and environment.yml for easy setup
>
> **Tech Stack**:
> - PyTorch 2.2 (MPS for Mac M1/M2/M3)
> - python-chess (chess logic and PGN)
> - Stockfish (optional, for analysis)
> - NumPy, Pandas, Matplotlib
>
> **How to Run**: `pip install -r requirements.txt` ‚Üí Run 4 notebooks in order ‚Üí Done!"

**Key Points**:
- Emphasize clean code and documentation
- Mention Mac optimization (MPS)

---

### Slide 11: Demo (1-2 minutes, optional)
**Visual**: Live or recorded screen capture

**Script**:
> "Let me show a quick demo:
> 1. Load trained model from weights/
> 2. Create neural agent with alpha-beta search
> 3. Play a few moves against Sunfish
> 4. Show policy logits for a position (top moves)
> 5. Show value evaluation changing over game
>
> [Show Jupyter notebook running live or pre-recorded]"

**Key Points**:
- Keep it brief (1-2 mins max)
- Pre-record if live demo risky

---

### Slide 12: Takeaways and Q&A (30 seconds)
**Visual**: Summary bullet points

**Script**:
> "**Summary**:
> - Built chess agent with PyTorch (policy + value network)
> - Trained via supervised learning on 100k Lichess positions
> - Integrated with alpha-beta search (policy ordering + value eval)
> - Benchmarked: +185 Elo vs Sunfish, competitive with Stockfish-Lv5
> - Reproducible on MacBook, complete notebooks and code
>
> **Thank you! Questions?"**

---

## Presentation Tips

### Timing Allocation (15-min target)
- Introduction: 1 min
- Problem + Data: 3 mins
- Model + Training: 4 mins
- Search + Benchmarks: 5 mins
- Insights + Future Work: 2 mins

### Visuals to Prepare
1. Phase distribution histogram
2. Training curves (loss + accuracy)
3. Match results bar chart
4. ACPL by phase chart
5. Sample PGN game (annotated)
6. Architecture diagram
7. Alpha-beta tree with policy ordering

### Rehearsal Checklist
- [ ] Practice with timer (aim for 12-13 mins to leave buffer)
- [ ] Test demo notebook (pre-record if needed)
- [ ] Prepare backup slides (in case questions on specific topics)
- [ ] Check all charts are legible on projector
- [ ] Have PGN viewer ready for sample game

### Common Questions to Anticipate
1. **"Why not full AlphaZero with self-play?"**
   - *Answer*: Computational constraints (no GPU cluster); supervised learning as baseline

2. **"How does policy accuracy relate to playing strength?"**
   - *Answer*: Not directly‚Äîtop-5 matters more; search amplifies policy quality

3. **"What's the bottleneck for stronger play?"**
   - *Answer*: Training data quantity and search depth (both fixable with more compute)

4. **"How long to train?"**
   - *Answer*: ~30 mins on M1 Mac for 10 epochs (100k positions)

5. **"Could this beat a human?"**
   - *Answer*: Yes, likely 1200-1400 Elo (beginner-intermediate); needs improvements for stronger play

---

## Final Notes

- **Confidence**: You built a working chess engine with measurable performance!
- **Storytelling**: Frame as journey from data ‚Üí model ‚Üí search ‚Üí benchmarks
- **Visuals**: Let charts speak‚Äîavoid walls of text
- **Enthusiasm**: Show passion for the problem and solution

Good luck with your presentation! üéì‚ôüÔ∏è

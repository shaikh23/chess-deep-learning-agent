{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chess Deep Learning Agent: Supervised Training\n",
    "\n",
    "Train policy and value networks via imitation learning from expert games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# NEW: Import shard-based dataset loader\n",
    "from data.shard_dataset import create_shard_dataloaders\n",
    "from model.nets import MLPPolicy, MLPPolicyValue, CNNPolicyValue, MiniResNetPolicyValue, initialize_weights\n",
    "from model.loss import PolicyValueLoss\n",
    "from utils.metrics import policy_top_k_accuracy\n",
    "from utils.plotting import plot_training_curves\n",
    "from utils.seeds import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Directories\n",
    "DATA_DIR = Path('../artifacts/data')\n",
    "SHARD_DIR = DATA_DIR / 'shards'  # NEW: Shard directory\n",
    "WEIGHTS_DIR = Path('../artifacts/weights')\n",
    "WEIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_DIR = Path('../artifacts/logs')\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FIGURES_DIR = Path('../reports/figures')\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Device: cpu\n",
      "Model: miniresnet (6×64)\n",
      "Value head: ENABLED\n",
      "Epochs: 10 (warmup: 2)\n",
      "Batch size: 256\n",
      "Learning rate: 0.001\n",
      "Policy smoothing (ε): 0.05\n",
      "Value loss weight (λ): 0.35\n",
      "AMP: True\n",
      "Channels-last: True\n",
      "Phase-balanced sampling: True\n",
      "Augmentation: True\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Training config - UPDATED for strength improvements\n",
    "CONFIG = {\n",
    "    # Model architecture\n",
    "    'model_type': 'miniresnet',  # 'mlp', 'cnn', 'miniresnet'\n",
    "    'num_blocks': 6,  # Consider 8 for more capacity with 1M+ data\n",
    "    'channels': 64,   # Consider 96 for more capacity with 1M+ data\n",
    "    'train_value_head': True,\n",
    "\n",
    "    # Training\n",
    "    'batch_size': 256,\n",
    "    'num_epochs': 10,  # CHANGED: Reduced from 20 (more data = fewer epochs needed)\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'lr_schedule': 'cosine_warmup',  # CHANGED: Added warmup\n",
    "    'warmup_epochs': 2,  # NEW: Warmup for first 2 epochs\n",
    "\n",
    "    # Loss - UPDATED values\n",
    "    'policy_smoothing': 0.05,\n",
    "    'value_weight': 0.35,  # CHANGED: from 0.7 to 0.35 (focus on policy first)\n",
    "\n",
    "    # Optimization - NEW\n",
    "    'use_amp': True,  # Automatic Mixed Precision for MPS\n",
    "    'channels_last': True,  # Memory format optimization for CNNs\n",
    "    'gradient_clip': 1.0,\n",
    "\n",
    "    # Data - NEW\n",
    "    'use_shards': True,  # Use shard-based loading\n",
    "    'phase_balanced': True,  # Phase-balanced sampling\n",
    "    'augment_train': True,  # Augmentation (file flips)\n",
    "\n",
    "    # Device\n",
    "    'device': 'mps' if torch.backends.mps.is_available() else 'cpu',\n",
    "}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print(f\"Model: {CONFIG['model_type']} ({CONFIG['num_blocks']}×{CONFIG['channels']})\")\n",
    "print(f\"Value head: {'ENABLED' if CONFIG['train_value_head'] else 'DISABLED'}\")\n",
    "print(f\"Epochs: {CONFIG['num_epochs']} (warmup: {CONFIG['warmup_epochs']})\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"Policy smoothing (ε): {CONFIG['policy_smoothing']}\")\n",
    "print(f\"Value loss weight (λ): {CONFIG['value_weight']}\")\n",
    "print(f\"AMP: {CONFIG['use_amp']}\")\n",
    "print(f\"Channels-last: {CONFIG['channels_last']}\")\n",
    "print(f\"Phase-balanced sampling: {CONFIG['phase_balanced']}\")\n",
    "print(f\"Augmentation: {CONFIG['augment_train']}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "device = torch.device(CONFIG['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<cell_type>markdown</cell_type>## Load Data (Shard-Based or CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from sharded data...\n",
      "======================================================================\n",
      "CREATING SHARD-BASED DATALOADERS\n",
      "======================================================================\n",
      "Found 57 shards in ../artifacts/data/shards/train\n",
      "Total positions: 2,829,318\n",
      "Found 18 shards in ../artifacts/data/shards/val\n",
      "Total positions: 893,556\n",
      "Creating phase-balanced sampler...\n",
      "Train loader: 11052 batches\n",
      "Val loader: 3491 batches\n",
      "Phase-balanced sampling: True\n",
      "Augmentation: True\n",
      "======================================================================\n",
      "✓ Loaded shard-based dataloaders\n",
      "  Train batches: 11052\n",
      "  Val batches: 3491\n"
     ]
    }
   ],
   "source": [
    "# Load data: Check if shards exist, otherwise fall back to CSV\n",
    "if CONFIG['use_shards'] and (SHARD_DIR / 'train').exists() and (SHARD_DIR / 'val').exists():\n",
    "    print(\"Loading from sharded data...\")\n",
    "    \n",
    "    train_loader, val_loader = create_shard_dataloaders(\n",
    "        train_shard_dir=SHARD_DIR / 'train',\n",
    "        val_shard_dir=SHARD_DIR / 'val',\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        num_workers=0,  # Mac compatibility\n",
    "        phase_balanced=CONFIG['phase_balanced'],\n",
    "        augment_train=CONFIG['augment_train'],\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Loaded shard-based dataloaders\")\n",
    "    print(f\"  Train batches: {len(train_loader)}\")\n",
    "    print(f\"  Val batches: {len(val_loader)}\")\n",
    "    \n",
    "elif (DATA_DIR / 'train.csv.gz').exists() and (DATA_DIR / 'val.csv.gz').exists():\n",
    "    print(\"Shards not found. Loading from CSV (legacy mode)...\")\n",
    "    print(\"⚠️  Warning: CSV mode uses smaller dataset (100k positions)\")\n",
    "    print(\"   For best results, run stream sampler to create shards with 1M+ positions\")\n",
    "    \n",
    "    from data.dataset import create_dataloaders\n",
    "    \n",
    "    train_df = pd.read_csv(DATA_DIR / 'train.csv.gz', compression='gzip')\n",
    "    val_df = pd.read_csv(DATA_DIR / 'val.csv.gz', compression='gzip')\n",
    "    \n",
    "    print(f\"Train: {len(train_df):,} positions\")\n",
    "    print(f\"Val:   {len(val_df):,} positions\")\n",
    "    \n",
    "    train_loader, val_loader = create_dataloaders(\n",
    "        train_df,\n",
    "        val_df,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        num_workers=0,\n",
    "        include_auxiliary=False,\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No training data found!\\n\"\n",
    "        f\"Expected either:\\n\"\n",
    "        f\"  1. Shards: {SHARD_DIR}/train/ and {SHARD_DIR}/val/\\n\"\n",
    "        f\"  2. CSV: {DATA_DIR}/train.csv.gz and {DATA_DIR}/val.csv.gz\\n\\n\"\n",
    "        f\"Run stream sampler to create shards:\\n\"\n",
    "        f\"  python -m src.data.stream_sampler --pgn-dir <path> --target 1000000\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using channels_last memory format\n",
      "\n",
      "Model: miniresnet\n",
      "Architecture: 6 blocks × 64 channels\n",
      "Value head: YES\n",
      "Parameters: 5,995,265\n"
     ]
    }
   ],
   "source": [
    "# Create model with value head support\n",
    "if CONFIG['model_type'] == 'mlp':\n",
    "    if CONFIG['train_value_head']:\n",
    "        model = MLPPolicyValue(\n",
    "            hidden_dims=(1024, 512, 512),\n",
    "            policy_head_hidden=512,\n",
    "            value_head_hidden=256,\n",
    "            dropout=0.3,\n",
    "        )\n",
    "    else:\n",
    "        model = MLPPolicy()\n",
    "elif CONFIG['model_type'] == 'cnn':\n",
    "    model = CNNPolicyValue(\n",
    "        num_channels=CONFIG['channels'],\n",
    "        num_layers=4,\n",
    "        policy_head_hidden=512,\n",
    "        value_head_hidden=256,\n",
    "    )\n",
    "elif CONFIG['model_type'] == 'miniresnet':\n",
    "    model = MiniResNetPolicyValue(\n",
    "        num_blocks=CONFIG['num_blocks'],\n",
    "        channels=CONFIG['channels'],\n",
    "        policy_head_hidden=512,\n",
    "        value_head_hidden=256,\n",
    "        dropout=0.1,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model type: {CONFIG['model_type']}\")\n",
    "\n",
    "initialize_weights(model)\n",
    "model = model.to(device)\n",
    "\n",
    "# NEW: Apply channels_last memory format for CNNs (faster on MPS)\n",
    "if CONFIG['channels_last'] and CONFIG['model_type'] in ['cnn', 'miniresnet']:\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "    print(\"✓ Using channels_last memory format\")\n",
    "\n",
    "print(f\"\\nModel: {CONFIG['model_type']}\")\n",
    "print(f\"Architecture: {CONFIG['num_blocks']} blocks × {CONFIG['channels']} channels\")\n",
    "print(f\"Value head: {'YES' if CONFIG['train_value_head'] else 'NO'}\")\n",
    "if hasattr(model, 'count_parameters'):\n",
    "    print(f\"Parameters: {model.count_parameters():,}\")\n",
    "else:\n",
    "    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using cosine schedule with 2-epoch warmup\n",
      "✓ Using Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aneesshaikh/projects/data_env/venv/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "criterion = PolicyValueLoss(\n",
    "    value_weight=CONFIG['value_weight'],\n",
    "    policy_smoothing=CONFIG['policy_smoothing'],\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay'],\n",
    ")\n",
    "\n",
    "# NEW: Learning rate scheduler with warmup\n",
    "if CONFIG['lr_schedule'] == 'cosine_warmup':\n",
    "    def warmup_cosine_schedule(epoch):\n",
    "        \"\"\"Warmup for first few epochs, then cosine decay.\"\"\"\n",
    "        if epoch < CONFIG['warmup_epochs']:\n",
    "            # Linear warmup\n",
    "            return (epoch + 1) / CONFIG['warmup_epochs']\n",
    "        else:\n",
    "            # Cosine annealing after warmup\n",
    "            progress = (epoch - CONFIG['warmup_epochs']) / (CONFIG['num_epochs'] - CONFIG['warmup_epochs'])\n",
    "            return 0.5 * (1.0 + np.cos(np.pi * progress))\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_cosine_schedule)\n",
    "    print(f\"✓ Using cosine schedule with {CONFIG['warmup_epochs']}-epoch warmup\")\n",
    "    \n",
    "elif CONFIG['lr_schedule'] == 'cosine':\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=CONFIG['num_epochs'],\n",
    "    )\n",
    "    print(\"✓ Using cosine annealing schedule\")\n",
    "    \n",
    "elif CONFIG['lr_schedule'] == 'step':\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=3,\n",
    "        gamma=0.5,\n",
    "    )\n",
    "    print(\"✓ Using step schedule\")\n",
    "else:\n",
    "    scheduler = None\n",
    "    print(\"⚠️  No LR schedule\")\n",
    "\n",
    "# NEW: AMP scaler for mixed precision training\n",
    "scaler = None\n",
    "if CONFIG['use_amp']:\n",
    "    try:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        print(\"✓ Using Automatic Mixed Precision (AMP)\")\n",
    "    except:\n",
    "        print(\"⚠️  AMP not available on this device\")\n",
    "        CONFIG['use_amp'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATED training functions with AMP and channels_last support\n",
    "def train_epoch(model, loader, criterion, optimizer, device, use_amp=False, scaler=None, channels_last=False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_policy_loss = 0.0\n",
    "    total_value_loss = 0.0\n",
    "    total_top1_acc = 0.0\n",
    "    total_top5_acc = 0.0\n",
    "\n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "        board, move_target, value_target = batch\n",
    "        board = board.to(device)\n",
    "        move_target = move_target.to(device)\n",
    "        value_target = value_target.to(device)\n",
    "        \n",
    "        # Apply channels_last if enabled\n",
    "        if channels_last:\n",
    "            board = board.to(memory_format=torch.channels_last)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass with optional AMP\n",
    "        if use_amp and scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(board, return_value=CONFIG['train_value_head'])\n",
    "                if isinstance(outputs, tuple):\n",
    "                    policy_logits, _, value_pred = outputs\n",
    "                else:\n",
    "                    policy_logits = outputs\n",
    "                    value_pred = None\n",
    "\n",
    "                loss, loss_dict = criterion(\n",
    "                    policy_logits,\n",
    "                    value_pred if CONFIG['train_value_head'] else None,\n",
    "                    move_target,\n",
    "                    value_target if CONFIG['train_value_head'] else None,\n",
    "                )\n",
    "\n",
    "            # Backward with scaling\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['gradient_clip'])\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            # Standard training without AMP\n",
    "            outputs = model(board, return_value=CONFIG['train_value_head'])\n",
    "            if isinstance(outputs, tuple):\n",
    "                policy_logits, _, value_pred = outputs\n",
    "            else:\n",
    "                policy_logits = outputs\n",
    "                value_pred = None\n",
    "\n",
    "            loss, loss_dict = criterion(\n",
    "                policy_logits,\n",
    "                value_pred if CONFIG['train_value_head'] else None,\n",
    "                move_target,\n",
    "                value_target if CONFIG['train_value_head'] else None,\n",
    "            )\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CONFIG['gradient_clip'])\n",
    "            optimizer.step()\n",
    "\n",
    "        # Metrics\n",
    "        total_loss += loss_dict['total_loss']\n",
    "        total_policy_loss += loss_dict['policy_loss']\n",
    "        if 'value_loss' in loss_dict:\n",
    "            total_value_loss += loss_dict['value_loss']\n",
    "\n",
    "        top1_acc = policy_top_k_accuracy(policy_logits, move_target, k=1)\n",
    "        top5_acc = policy_top_k_accuracy(policy_logits, move_target, k=5)\n",
    "        total_top1_acc += top1_acc\n",
    "        total_top5_acc += top5_acc\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'top1': f\"{top1_acc:.3f}\",\n",
    "        })\n",
    "\n",
    "    n = len(loader)\n",
    "    return {\n",
    "        'loss': total_loss / n,\n",
    "        'policy_loss': total_policy_loss / n,\n",
    "        'value_loss': total_value_loss / n if total_value_loss > 0 else 0.0,\n",
    "        'top1_acc': total_top1_acc / n,\n",
    "        'top5_acc': total_top5_acc / n,\n",
    "    }\n",
    "\n",
    "def evaluate(model, loader, criterion, device, channels_last=False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_top1_acc = 0.0\n",
    "    total_top3_acc = 0.0  # Added top-3 for better tracking\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            board, move_target, value_target = batch\n",
    "            board = board.to(device)\n",
    "            move_target = move_target.to(device)\n",
    "            value_target = value_target.to(device)\n",
    "            \n",
    "            if channels_last:\n",
    "                board = board.to(memory_format=torch.channels_last)\n",
    "\n",
    "            outputs = model(board, return_value=CONFIG['train_value_head'])\n",
    "            if isinstance(outputs, tuple):\n",
    "                policy_logits, _, value_pred = outputs\n",
    "            else:\n",
    "                policy_logits = outputs\n",
    "                value_pred = None\n",
    "\n",
    "            loss, loss_dict = criterion(\n",
    "                policy_logits,\n",
    "                value_pred if CONFIG['train_value_head'] else None,\n",
    "                move_target,\n",
    "                value_target if CONFIG['train_value_head'] else None,\n",
    "            )\n",
    "\n",
    "            total_loss += loss_dict['total_loss']\n",
    "            top1_acc = policy_top_k_accuracy(policy_logits, move_target, k=1)\n",
    "            top3_acc = policy_top_k_accuracy(policy_logits, move_target, k=3)\n",
    "            total_top1_acc += top1_acc\n",
    "            total_top3_acc += top3_acc\n",
    "\n",
    "    n = len(loader)\n",
    "    return {\n",
    "        'loss': total_loss / n,\n",
    "        'top1_acc': total_top1_acc / n,\n",
    "        'top3_acc': total_top3_acc / n,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING TRAINING\n",
      "======================================================================\n",
      "\n",
      "=== Epoch 1/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                                                                                         | 0/11052 [00:00<?, ?it/s]/Users/aneesshaikh/projects/data_env/venv/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Training:   0%|▏                                                                                                                  | 19/11052 [14:17<134:38:08, 43.93s/it, loss=7.8936, top1=0.004]"
     ]
    }
   ],
   "source": [
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "    'learning_rates': [],  # NEW: Track learning rate\n",
    "}\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train\n",
    "for epoch in range(1, CONFIG['num_epochs'] + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{CONFIG['num_epochs']} ===\")\n",
    "\n",
    "    # Train\n",
    "    train_metrics = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device,\n",
    "        use_amp=CONFIG['use_amp'],\n",
    "        scaler=scaler,\n",
    "        channels_last=CONFIG['channels_last']\n",
    "    )\n",
    "    print(f\"Train - Loss: {train_metrics['loss']:.4f}, \"\n",
    "          f\"Top-1: {train_metrics['top1_acc']:.4f}, \"\n",
    "          f\"Top-5: {train_metrics['top5_acc']:.4f}\")\n",
    "\n",
    "    # Validate\n",
    "    val_metrics = evaluate(model, val_loader, criterion, device, channels_last=CONFIG['channels_last'])\n",
    "    print(f\"Val   - Loss: {val_metrics['loss']:.4f}, \"\n",
    "          f\"Top-1: {val_metrics['top1_acc']:.4f}, \"\n",
    "          f\"Top-3: {val_metrics['top3_acc']:.4f}\")\n",
    "\n",
    "    # Update history\n",
    "    history['train_loss'].append(train_metrics['loss'])\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    history['train_acc'].append(train_metrics['top1_acc'])\n",
    "    history['val_acc'].append(val_metrics['top1_acc'])\n",
    "    history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    # Save best model\n",
    "    if val_metrics['top1_acc'] > best_val_acc:\n",
    "        best_val_acc = val_metrics['top1_acc']\n",
    "        torch.save(model.state_dict(), WEIGHTS_DIR / 'best_model.pth')\n",
    "        print(f\"✓ Saved best model (val_acc={best_val_acc:.4f})\")\n",
    "\n",
    "    # LR schedule\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "        print(f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), WEIGHTS_DIR / 'final_model.pth')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
    "print(f\"Model saved to: {WEIGHTS_DIR / 'best_model.pth'}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves(\n",
    "    history['train_loss'],\n",
    "    history['val_loss'],\n",
    "    history['train_acc'],\n",
    "    history['val_acc'],\n",
    "    save_path=FIGURES_DIR / 'training_curves.png',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Training Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training log and history\n",
    "log = {\n",
    "    'config': CONFIG,\n",
    "    'history': history,\n",
    "    'best_val_acc': best_val_acc,\n",
    "}\n",
    "\n",
    "with open(LOGS_DIR / 'training_log.json', 'w') as f:\n",
    "    json.dump(log, f, indent=2)\n",
    "\n",
    "# Also save history separately for easy loading in report notebook\n",
    "with open(LOGS_DIR / 'training_history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(\"✓ Saved training log\")\n",
    "print(f\"✓ Best validation accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"✓ Model saved to: {WEIGHTS_DIR / 'best_model.pth'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "✓ Trained policy+value network\n",
    "✓ Achieved top-1 accuracy: {best_val_acc:.2%}\n",
    "✓ Saved best model to artifacts/weights/\n",
    "\n",
    "**Next**: Notebook 03 - Integrate search and test playing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
